{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"81945647-5953-4666-8697-793d9660da77","showTitle":false,"title":""}},"source":["# **Linear Regression with Spark**\n","#### The assignment concerns a standard supervised learning pipeline, using part of [Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/) (You can find in [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD)). Our objective is to train a linear regression (LR) model that can anticipate the release year of a song, based on a given set of audio features.\n","\n","#### The assignment will comprise the following sections:\n","+  ####*Part 1:* Reading & parsing the original dataset (1 point)\n"," + #### *Vis. 1:* Features\n"," + #### *Vis. 2:* Shifting labels\n","+  ####*Part 2:* Development and assessment of a baseline model (1 point)\n"," + #### *Vis. 3:* Comparison of predicted vs. actual results\n","+  ####*Part 3:* Training with gradient descent and evaluatation of a LR model (2 points)\n"," + #### *Vis. 4:* Examination of training errors\n","+  ####*Part 4:* Training with MLlib and optimization of hyperparameters using grid search (2 points)\n"," + #### *Vis. 5:* Predictions of best model\n"," + #### *Vis. 6:* Heat map representation of hyperparameter\n","+  ####*Part 5:* Adding interactions between the features (2 points)\n"," \n","#### Please note that the relevant Spark methods can be found in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html?highlight=pyspark%20rdd#pyspark.RDD), while the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)Reference provides details of relevant NumPy methods, for reference."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1cb8480f-c5f4-43cf-a2db-1aa67fc61e8c","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["lable = 'comp4651_assignment_3'"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"18c6577b-9b18-410f-847d-e6298d5f58f6","showTitle":false,"title":""}},"source":["### Part 1: Reading & parsing the original dataset (1 point)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8cf820cb-d726-4e96-befc-309ceaf60aed","showTitle":false,"title":""}},"source":["#### (1-I) Data Loading and Verification\n","\n","#### The present location of the authentic information is a textual document. Our first move is to transform this unprocessed information into an RDD format, where each element of the RDD signifies a comma-separated string. The first segment of each unit is denoted by a label, representing a year, and followed by numeric audio features. Utilize the [count method](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html?highlight=pyspark%20rdd%20count#pyspark.RDD.count) to verify the number of data points available. Then, use the [take method](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.take.html?highlight=pyspark%20rdd%20take#pyspark.RDD.take) to generate and print a list of the first 5 data points in their original string format."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f9b87121-08c5-482f-bdf3-757e16fddf71","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# load testing library\n","import os.path\n","fileName = 'dbfs:/databricks-datasets/cs190/data-001/millionsong.txt'\n","\n","numPartitions = 2\n","rawData = sc.textFile(fileName, numPartitions)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4460c6ba-a348-48c4-a790-2f4e239148df","showTitle":false,"title":""}},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","numPoints = <FILL IN>\n","print(numPoints)\n","samplePoints = <FILL IN>\n","print(samplePoints)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"02a68f07-490f-4724-880b-133901a29e37","showTitle":false,"title":""}},"source":["#### (1-II) Store labeled traing instances\n","#### To store the labeled training instances in MLlib, we use [LabeledPoint](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.regression.LabeledPoint.html?highlight=pyspark%20mllib%20regression%20labeledpoint#pyspark.mllib.regression.LabeledPoint) object. We can generate a parsePoint function that takes a raw data point as input, parsing by applying Python's [unicode.split](https://docs.python.org/2/library/string.html#string.split) method to it, and then returns a `LabeledPoint` object. This parsePoint function can be used to parse samplePoints (mentioned in the previous question). Then print out the features and label of the 1st training point using the `LabeledPoint.features` and `LabeledPoint.label` attributes. Finally, we can compute the number of features in this dataset.\n","\n","#### It is noteworthy that `split()` can be called directly on a `unicode` or `str` object. E.g., `u'split,me'.split(',')` returns `[u'split', u'me']`."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"58f0fb2e-6a84-4aa8-8030-c9d9e1a42658","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["from pyspark.mllib.regression import LabeledPoint\n","import numpy as np\n","\n","# Here is a sample raw data point:\n","# '2001.0,0.884,0.610,0.600,0.474,0.247,0.357,0.344,0.33,0.600,0.425,0.60,0.419'\n","# In this raw data point, 2001.0 is the label, and the remaining values are features"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"40e486f1-ae5f-4020-bb5f-0fbb3d480e57","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","def parsePoint(line):\n","    \"\"\"Converts a comma separated unicode string into a `LabeledPoint`.\n","\n","    Args:\n","        line (unicode): Comma separated unicode string where the first element is the label and the\n","            remaining elements are features.\n","\n","    Returns:\n","        LabeledPoint: The line is converted into a `LabeledPoint`, which consists of a label and\n","            features.\n","    \"\"\"\n","    <FILL IN>\n","\n","parsedSamplePoints = <FILL IN>\n","firstPointFeatures = <FILL IN>\n","firstPointLabel = <FILL IN>\n","print(firstPointFeatures, firstPointLabel)\n","\n","d = len(firstPointFeatures)\n","print(d)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"59ea7277-ac5b-4310-9e36-6d3bbe74496b","showTitle":false,"title":""}},"source":["#### Vis. 1: Features\n","#### Initially, we need to import and configure the visualization library. Next, we intend to examine the crude feature of 50 data points by creating a heatmap that illustrates each feature in a grayscale and displays the variability of each feature among the 50 sample data points. All the features range from 0 to 1, where features closer to 1 are represented with darker shades of grey."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"aa2fa8df-9078-4021-aee3-6c049227ed18","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","\n","sampleMorePoints = rawData.take(50)\n","# You can uncomment the line below to see randomly selected features.  These will be randomly\n","# selected each time you run the cell.  Note that you should run this cell with the line commented\n","# out when answering the lab quiz questions.\n","# sampleMorePoints = rawData.takeSample(False, 50)\n","\n","\n","parsedSampleMorePoints = map(parsePoint, sampleMorePoints)\n","dataValues = list(map(lambda lp: lp.features.toArray(), parsedSampleMorePoints))\n","\n","def preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n","                gridWidth=1.0):\n","    \"\"\"Template for generating the plot layout.\"\"\"\n","    plt.close()\n","    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n","    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n","    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n","        axis.set_ticks_position('none')\n","        axis.set_ticks(ticks)\n","        axis.label.set_color('#999999')\n","        if hideLabels: axis.set_ticklabels([])\n","    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n","    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n","    return fig, ax\n","\n","# generate layout and plot\n","fig, ax = preparePlot(np.arange(.5, 11, 1), np.arange(.5, 49, 1), figsize=(8,7), hideLabels=True,\n","                      gridColor='#eeeeee', gridWidth=1.1)\n","print(type(dataValues))\n","image = plt.imshow(dataValues, interpolation='nearest', aspect='auto', cmap=cm.Greys)\n","for x, y, s in zip(np.arange(-.125, 12, 1), np.repeat(-.75, 12), [str(x) for x in range(12)]):\n","    plt.text(x, y, s, color='#999999', size='10')\n","plt.text(4.7, -3, 'Feature', color='#999999', size='11'), ax.set_ylabel('Observation')\n","pass"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f0839053-642e-4592-bb64-83fdc2dc1de8","showTitle":false,"title":""}},"source":["#### (1-III) Explore the range \n","#### Our task is to examine the labels and determine the period during which the songs were released. To accomplish this, we need to parse each element of the `rawData` RDD and then find the minimum and maximum labels."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ff71a5e0-6964-40a1-b43a-56c54849ce6c","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","parsedDataInit = rawData.map(<FILL IN>)\n","onlyLabels = parsedDataInit.map(<FILL IN>)\n","minYear = <FILL IN>\n","maxYear = <FILL IN>\n","print(maxYear, minYear)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d036cbf8-1389-446e-8aad-7b33413e2f48","showTitle":false,"title":""}},"source":["#### (1-IV) Shifting labels\n","#### As we just saw, the labels represent years in the 1900s and 2000s. In machine learning tasks, it's common to shift labels so that they begin from zero. Starting with `parsedDataInit`, we can create a new RDD of `LabeledPoint` objects, with the labels shifted such that the smallest label is zero."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"80ab8717-8427-4f0b-b566-4e8817684d92","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","parsedData = parsedDataInit.<FILL IN>\n","\n","# Should be a LabeledPoint\n","print(type(parsedData.take(1)[0]))\n","# View the first point\n","print('\\n{0}'.format(parsedData.take(1)))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f449429a-2e92-4e93-9736-59a6ce9151f1","showTitle":false,"title":""}},"source":["#### Vis. 2: Shifting labels\n","#### We will compare the distribution of labels before and after shifting them by plotting two scatter plots. The tuples in both plots store i) a label value, and ii) the number of training points with this label. While the first scatter plot displays the initial labels, the second one visualizes the shifted labels. It's worth noting that both plots look identical, except for the labels on the x-axis."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4fa4c2bc-b7af-4aba-9450-c37a81de10ac","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# get data for plot\n","oldData = (parsedDataInit\n","           .map(lambda lp: (lp.label, 1))\n","           .reduceByKey(lambda x, y: x + y)\n","           .collect())\n","x, y = zip(*oldData)\n","\n","# generate layout and plot data\n","fig, ax = preparePlot(np.arange(1920, 2050, 20), np.arange(0, 150, 20))\n","plt.scatter(x, y, s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n","ax.set_xlabel('Year'), ax.set_ylabel('Count')\n","pass"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9c18f227-cac2-40a2-8478-66bc6c21efc0","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# get data for plot\n","newData = (parsedData\n","           .map(lambda lp: (lp.label, 1))\n","           .reduceByKey(lambda x, y: x + y)\n","           .collect())\n","x, y = zip(*newData)\n","\n","# generate layout and plot data\n","fig, ax = preparePlot(np.arange(0, 120, 20), np.arange(0, 120, 20))\n","plt.scatter(x, y, s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n","ax.set_xlabel('Year (shifted)'), ax.set_ylabel('Count')\n","pass"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0c34283b-0e46-48d5-9120-e5ea0db045f8","showTitle":false,"title":""}},"source":["#### (1-V) Training set, validation set, and test set\n","#### Our last step in parsing the dataset involves splitting it into training, validation, and test sets. We can use the [randomSplit method](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.randomSplit.html?highlight=pyspark%20rdd%20randomsplit#pyspark.RDD.randomSplit) with predefined weights and a seed to create RDDs for each set. It's essential to cache each of these RDDs as we'll need to access them multiple times later. Finally, we should compute the size of each set and verify that their sum is equal to the total size of the dataset from Part (1-I)."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"09afdc9b-e982-489d-b8b0-7beadfe4b40c","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","weights = [.8, .1, .1]\n","seed = 42\n","parsedTrainData, parsedValData, parsedTestData = parsedData.<FILL IN>\n","parsedTrainData.<FILL IN>\n","parsedValData.<FILL IN>\n","parsedTestData.<FILL IN>\n","nTrain = parsedTrainData.<FILL IN>\n","nVal = parsedValData.<FILL IN>\n","nTest = parsedTestData.<FILL IN>\n","\n","print(nTrain, nVal, nTest, nTrain + nVal + nTest)\n","print(parsedData.count())"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f6d090e7-6304-4ede-8027-ca513e8dde07","showTitle":false,"title":""}},"source":["### Part 2:  Development and assessment of a baseline model (1 point)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ceaab2a1-d2e2-4ee0-a26b-2b5be19c9f60","showTitle":false,"title":""}},"source":["#### (2-I) Average label\n","#### A straightforward baseline model is one where we make the same prediction regardless of the input data. To achieve this, we can use the average label in the training set as the constant prediction value. We can compute this value by using an appropriate method in the [RDD API](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html?highlight=pyspark%20rdd#pyspark.RDD)."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"df87d8be-229b-4c7c-b7f8-ab8d2f8e5dff","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","averageTrainYear = (parsedTrainData\n","                    <FILL IN>)\n","print(averageTrainYear)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b2b88df6-1155-43e9-9051-b19e67a7e79d","showTitle":false,"title":""}},"source":["#### (2-II) Root mean squared error\n","#### It's important to evaluate how effective the simple baseline model is, and one commonly used metric for this is the root mean squared error ([RMSE](http://en.wikipedia.org/wiki/Root-mean-square_deviation)). To calculate the RMSE, we need a function that takes in an RDD of (label, prediction) tuples as input, and test out this function on an example."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1f6eb87f-1370-4a27-9edd-c4a84868898e","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","def squaredError(label, prediction):\n","    \"\"\"Calculates the the squared error for a single prediction.\n","\n","    Args:\n","        label (float): The correct value for this observation.\n","        prediction (float): The predicted value for this observation.\n","\n","    Returns:\n","        float: The difference between the `label` and `prediction` squared.\n","    \"\"\"\n","    <FILL IN>\n","\n","def calcRMSE(labelsAndPreds):\n","    \"\"\"Calculates the root mean squared error for an `RDD` of (label, prediction) tuples.\n","\n","    Args:\n","        labelsAndPred (RDD of (float, float)): An `RDD` consisting of (label, prediction) tuples.\n","\n","    Returns:\n","        float: The square root of the mean of the squared errors.\n","    \"\"\"\n","    <FILL IN>\n","\n","labelsAndPreds = sc.parallelize([(3., 1.), (1., 2.), (2., 2.)])\n","# RMSE = sqrt[((3-1)^2 + (1-2)^2 + (2-2)^2) / 3] = 1.291\n","exampleRMSE = calcRMSE(labelsAndPreds)\n","print(exampleRMSE)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3680aefb-9d85-4b38-b966-0cc5e97468f3","showTitle":false,"title":""}},"source":["#### (2-III) RMSE of Training, validation and test set\n","#### Let's now compute the root mean squared error (RMSE) of our baseline model on the training, validation, and test datasets. First, we need to create RDDs that contain tuples of (label, prediction) for each dataset. Then, we can use the `calcRMSE` function to compute the RMSE for each dataset. It is worth noting that the RMSE represents the average prediction error in terms of years for each dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3af6b575-f70c-4a8a-a222-3e53394cea40","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","labelsAndPredsTrain = parsedTrainData.<FILL IN>\n","rmseTrainBase = <FILL IN>\n","\n","labelsAndPredsVal = parsedValData.<FILL IN>\n","rmseValBase = <FILL IN>\n","\n","labelsAndPredsTest = parsedTestData.<FILL IN>\n","rmseTestBase = <FILL IN>\n","\n","print('Baseline Train RMSE = {0:.3f}'.format(rmseTrainBase))\n","print('Baseline Validation RMSE = {0:.3f}'.format(rmseValBase))\n","print('Baseline Test RMSE = {0:.3f}'.format(rmseTestBase))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3d101f6c-4634-4027-bf21-2960b505e729","showTitle":false,"title":""}},"source":["#### Vis. 3: Comparison of predicted vs. actual results\n","#### We will generate scatter plots to visualize predictions on the validation dataset. These plots will display tuples that contain i) the predicted value and ii) the true label. The first scatter plot represents the perfect scenario where the predicted value exactly matches the true label. However, in the second plot, we use the baseline predictor (i.e., `averageTrainYear`) for all predicted values. Additionally, note that the scatter plots use a color scheme ranging from light yellow for instances where the true and predicted values are equal, to bright red for instances where they drastically differ."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"19a04f15-e3c9-4c62-8828-68d526cbbd47","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["from matplotlib.colors import ListedColormap, Normalize\n","from matplotlib.cm import get_cmap\n","cmap = get_cmap('YlOrRd')\n","norm = Normalize()\n","\n","actual = np.asarray(parsedValData\n","                    .map(lambda lp: lp.label)\n","                    .collect())\n","error = np.asarray(parsedValData\n","                   .map(lambda lp: (lp.label, lp.label))\n","                   .map(lambda lp_tuple: squaredError(lp_tuple[0], lp_tuple[1]))\n","                   .collect())\n","clrs = cmap(np.asarray(norm(error)))[:,0:3]\n","\n","fig, ax = preparePlot(np.arange(0, 100, 20), np.arange(0, 100, 20))\n","plt.scatter(actual, actual, s=14**2, c=clrs, edgecolors='#888888', alpha=0.75, linewidths=0.5)\n","ax.set_xlabel('Predicted'), ax.set_ylabel('Actual')\n","pass"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a1065a2e-c0ea-497b-b7ec-89f77cf8557d","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["predictions = np.asarray(parsedValData\n","                         .map(lambda lp: averageTrainYear)\n","                         .collect())\n","error = np.asarray(parsedValData\n","                   .map(lambda lp: (lp.label, averageTrainYear))\n","                   .map(lambda lp_tuple: squaredError(lp_tuple[0], lp_tuple[1]))\n","                   .collect())\n","norm = Normalize()\n","clrs = cmap(np.asarray(norm(error)))[:,0:3]\n","\n","fig, ax = preparePlot(np.arange(53.0, 55.0, 0.5), np.arange(0, 100, 20))\n","ax.set_xlim(53, 55)\n","plt.scatter(predictions, actual, s=14**2, c=clrs, edgecolors='#888888', alpha=0.75, linewidths=0.3)\n","ax.set_xlabel('Predicted'), ax.set_ylabel('Actual')"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"042c3770-0f05-4349-853b-f2284b62fbcf","showTitle":false,"title":""}},"source":["### Part 3: Training with gradient descent and evaluatation of a LR model (2 points)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8711dac4-6ced-497f-a689-b96266c60dfc","showTitle":false,"title":""}},"source":["#### (3-I) Gradient summand\n","#### Let's attempt to improve our predictions using linear regression with gradient descent (omit the intercept for now). As a reminder, the gradient descent update rule for linear regression is: $$ \\scriptsize \\mathbf{w}_{i+1} = \\mathbf{w}_i - \\alpha_i \\sum_j (\\mathbf{w}_i^\\top\\mathbf{x}_j  - y_j) \\mathbf{x}_j \\,.$$ where i is the iteration number of the gradient descent algorithm, and j identifies the observation.\n","#### First, implement a function that gives the summand for this update, i.e., the summand equals $$ \\scriptsize (\\mathbf{w}^\\top \\mathbf{x} - y) \\mathbf{x} \\$$ Then you should test this function on two examples.  Using `DenseVector` [dot](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.linalg.DenseVector.html?highlight=pyspark%20mllib%20linalg%20densevector%20dot#pyspark.mllib.linalg.DenseVector.dot) method."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"22e0a639-cff1-4084-9373-8ac9884e67e9","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["from pyspark.mllib.linalg import DenseVector"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"53fec63f-30d0-4403-9071-1487d6f445bc","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","def gradientSummand(weights, lp):\n","    \"\"\"Calculates the gradient summand for a given weight and `LabeledPoint`.\n","\n","    Note:\n","        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n","        within this function.  For example, they both implement the `dot` method.\n","\n","    Args:\n","        weights (DenseVector): An array of model weights (betas).\n","        lp (LabeledPoint): The `LabeledPoint` for a single observation.\n","\n","    Returns:\n","        DenseVector: An array of values the same length as `weights`.  The gradient summand.\n","    \"\"\"\n","    <FILL IN>\n","\n","exampleW = DenseVector([1, 1, 1])\n","exampleLP = LabeledPoint(2.0, [3, 1, 4])\n","# gradientSummand = (dot([1 1 1], [3 1 4]) - 2) * [3 1 4] = (8 - 2) * [3 1 4] = [18 6 24]\n","summandOne = gradientSummand(exampleW, exampleLP)\n","print(summandOne)\n","\n","exampleW = DenseVector([.24, 1.2, -1.4])\n","exampleLP = LabeledPoint(3.0, [-1.4, 4.2, 2.1])\n","summandTwo = gradientSummand(exampleW, exampleLP)\n","print(summandTwo)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1031422e-bfa5-4618-a8c2-67f83b0d3157","showTitle":false,"title":""}},"source":["#### (3-II) Make predictions using weights\n","#### Subsequently, create a function called `getLabeledPredictions` that takes the weights and a `LabeledPoint` observation and returns a tuple of the form (label, prediction). It is worth noting that the prediction can be obtained by calculating the dot product of the weights with the features of the observation."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b11cd9c9-7a56-4ed6-aa88-5f6514000d7f","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","def getLabeledPrediction(weights, observation):\n","    \"\"\"Calculates predictions and returns a (label, prediction) tuple.\n","\n","    Note:\n","        The labels should remain unchanged as we'll use this information to calculate prediction\n","        error later.\n","\n","    Args:\n","        weights (np.ndarray): An array with one weight for each features in `trainData`.\n","        observation (LabeledPoint): A `LabeledPoint` that contain the correct label and the\n","            features for the data point.\n","\n","    Returns:\n","        tuple: A (label, prediction) tuple.\n","    \"\"\"\n","    return <FILL IN>\n","\n","weights = np.array([1.0, 1.5])\n","predictionExample = sc.parallelize([LabeledPoint(2, np.array([1.0, .5])),\n","                                    LabeledPoint(1.5, np.array([.5, .5]))])\n","labelsAndPredsExample = predictionExample.map(lambda lp: getLabeledPrediction(weights, lp))\n","print(labelsAndPredsExample.collect())"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"195b6200-845f-4bc1-b032-80114259df42","showTitle":false,"title":""}},"source":["#### (3-III) Gradient descent\n","#### Next, give a gradient descent function for LR and test on an example."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a98fe0d8-29ce-4856-a346-af2324fe1c0f","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","def linregGradientDescent(trainData, numIters):\n","    \"\"\"Calculates the weights and error for a linear regression model trained with gradient descent.\n","\n","    Note:\n","        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n","        within this function.  For example, they both implement the `dot` method.\n","\n","    Args:\n","        trainData (RDD of LabeledPoint): The labeled data for use in training the model.\n","        numIters (int): The number of iterations of gradient descent to perform.\n","\n","    Returns:\n","        (np.ndarray, np.ndarray): A tuple of (weights, training errors).  Weights will be the\n","            final weights (one weight per feature) for the model, and training errors will contain\n","            an error (RMSE) for each iteration of the algorithm.\n","    \"\"\"\n","    # The length of the training data\n","    n = trainData.count()\n","    # The number of features in the training data\n","    d = len(trainData.take(1)[0].features)\n","    w = np.zeros(d)\n","    alpha = 1.0\n","    # We will compute and store the training error after each iteration\n","    errorTrain = np.zeros(numIters)\n","    for i in range(numIters):\n","        # Use getLabeledPrediction from (3b) with trainData to obtain an RDD of (label, prediction)\n","        # tuples.  Note that the weights all equal 0 for the first iteration, so the predictions will\n","        # have large errors to start.\n","        labelsAndPredsTrain = trainData.<FILL IN>\n","        errorTrain[i] = calcRMSE(labelsAndPredsTrain)\n","\n","        # Calculate the `gradient`.  Make use of the `gradientSummand` function you wrote in (3a).\n","        # Note that `gradient` sould be a `DenseVector` of length `d`.\n","        gradient = <FILL IN>\n","\n","        # Update the weights\n","        alpha_i = alpha / (n * np.sqrt(i+1))\n","        w -= <FILL IN>\n","    return w, errorTrain\n","\n","# create a toy dataset with n = 10, d = 3, and then run 5 iterations of gradient descent\n","# note: the resulting model will not be useful; the goal here is to verify that\n","# linregGradientDescent is working properly\n","exampleN = 10\n","exampleD = 3\n","exampleData = (sc\n","               .parallelize(parsedTrainData.take(exampleN))\n","               .map(lambda lp: LabeledPoint(lp.label, lp.features[0:exampleD])))\n","print(exampleData.take(2))\n","exampleNumIters = 5\n","exampleWeights, exampleErrorTrain = linregGradientDescent(exampleData, exampleNumIters)\n","print(exampleWeights)\n","print(exampleErrorTrain)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"86d966e0-5c57-4690-92ab-60361449e993","showTitle":false,"title":""}},"source":["#### (3-IV) Train the model\n","#### Let's proceed to train a linear regression model on the entire training dataset and then check its accuracy on the validation set. However, it's important to note that we shouldn't evaluate the model on the test set at this stage since doing so could introduce bias in our final results. \n","#### We have already completed most of the necessary steps, such as computing the number of features in Part (1â€”II), creating the training and validation datasets, and determining their sizes in Part (1-V), and also writing a function to calculate RMSE in Part (2-II)."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"665455d7-3230-4c8e-b5cd-d359b2a2856b","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","numIters = 50\n","weightsLR0, errorTrainLR0 = linregGradientDescent(<FILL IN>)\n","\n","labelsAndPreds = parsedValData.<FILL IN>\n","rmseValLR0 = calcRMSE(labelsAndPreds)\n","\n","print('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}'.format(rmseValBase, rmseValLR0))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7b3cac21-f7e8-482d-80e8-df22cc5b08c8","showTitle":false,"title":""}},"source":["#### Vis. 4: Examination of training errors\n","#### We will examine how the training error changes over iterations by taking the logarithm of the error values. The first scatter plot displays the logarithm of the training error for all 50 iterations. The second plot emphasizes the final 44 iterations and displays the training error directly."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9531c4f3-7095-49f2-b729-7fe5917cf752","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["norm = Normalize()\n","clrs = cmap(np.asarray(norm(np.log(errorTrainLR0))))[:,0:3]\n","\n","fig, ax = preparePlot(np.arange(0, 60, 10), np.arange(2, 6, 1))\n","ax.set_ylim(2, 6)\n","plt.scatter(range(0, numIters), np.log(errorTrainLR0), s=14**2, c=clrs, edgecolors='#888888', alpha=0.75)\n","ax.set_xlabel('Iteration'), ax.set_ylabel(r'$\\log_e(errorTrainLR0)$')\n","pass"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2895c2a2-47dc-4d41-bda4-72dfc6ceb643","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["norm = Normalize()\n","clrs = cmap(np.asarray(norm(errorTrainLR0[6:])))[:,0:3]\n","\n","fig, ax = preparePlot(np.arange(0, 60, 10), np.arange(17, 22, 1))\n","ax.set_ylim(17.8, 21.2)\n","plt.scatter(range(0, numIters-6), errorTrainLR0[6:], s=14**2, c=clrs, edgecolors='#888888', alpha=0.75)\n","ax.set_xticklabels(map(str, range(6, 66, 10)))\n","ax.set_xlabel('Iteration'), ax.set_ylabel(r'Training Error')\n","pass"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"16be68b1-9c31-469a-a52d-f6ddadb87b65","showTitle":false,"title":""}},"source":["### Part 4: Training with MLlib and optimization of hyperparameters using grid search (2 points)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8e3accd2-ae15-4525-81e0-040acb3435fd","showTitle":false,"title":""}},"source":["#### (4-I) SGD\n","#### We have already achieved better performance than the baseline model, however, we will attempt to further improve by introducing an intercept, regularization, and increasing the number of iterations. MLlib's [LinearRegressionWithSGD](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.regression.LinearRegressionWithSGD.html?highlight=pyspark%20mllib%20regression%20linearregressionwithsgd#pyspark.mllib.regression.LinearRegressionWithSGD) implements a similar algorithm to what we implemented in Part (3-II), but with more features such as stochastic gradient approximation, the option to include an intercept in the model, and also allowing L1 or L2 regularization. We will start by training a model with LinearRegressionWithSGD that includes an intercept and L2 regularization. This will result in a [LinearRegressionModel](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.regression.LinearRegressionModel.html?highlight=pyspark%20mllib%20regression%20linearregressionmodel#pyspark.mllib.regression.LinearRegressionModel).  Next, . Then, we can use the model's [weights](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.regression.LinearRegressionModel.html?highlight=pyspark%20mllib%20regression%20linearregressionmodel%20weights#pyspark.mllib.regression.LinearRegressionModel.weights)  and [intercept](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.regression.LinearRegressionModel.html?highlight=pyspark%20mllib%20regression%20linearregressionmodel%20intercept#pyspark.mllib.regression.LinearRegressionModel.intercept)  attributes to print out the model's parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"829212fb-7d4e-48ba-99b3-114d076beac7","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["from pyspark.mllib.regression import LinearRegressionWithSGD\n","# Values to use when training the linear regression model\n","numIters = 500  # iterations\n","alpha = 1.0  # step\n","miniBatchFrac = 1.0  # miniBatchFraction\n","reg = 1e-1  # regParam\n","regType = 'l2'  # regType\n","useIntercept = True  # intercept"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d3be931e-101b-4424-94b4-eca4c82b6ab9","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","firstModel = LinearRegressionWithSGD.<FILL IN>\n","\n","# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n","weightsLR1 = <FILL IN>\n","interceptLR1 = <FILL IN>\n","print(weightsLR1, interceptLR1)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0699e915-1731-49a9-a45a-5b282b9ab5ce","showTitle":false,"title":""}},"source":["#### (4-II) Prediction\n","#### Now use the [LinearRegressionModel.predict()](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.regression.LinearRegressionModel.html?highlight=pyspark%20mllib%20regression%20linearregressionmodel%20predict#pyspark.mllib.regression.LinearRegressionModel.predict) method to make a prediction on a sample point by feeding in the `features` from a `LabeledPoint`."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"72f1f6cc-7ca0-41eb-80ac-b73e0c03a4f1","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","samplePoint = parsedTrainData.take(1)[0]\n","samplePrediction = <FILL IN>\n","print(samplePrediction)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6209a03a-4c76-4a50-91c9-650667e5a849","showTitle":false,"title":""}},"source":["#### (4-III) Evaluate RMSE\n","#### Subsequently, assess the performance of this model on the validation dataset. Generate an RDD called `labelsAndPreds` by applying the `predict()`  method to the validation dataset. Then, use the `calcRMSE()` function from Part (2-II) to calculate the root-mean-squared error (RMSE) of the model's predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"700e0e2c-ae97-452a-9ab8-108c2f672d4a","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","labelsAndPreds = <FILL IN>\n","rmseValLR1 = <FILL IN>\n","\n","print(('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}' + '\\n\\tLR1 = {2:.3f}').format(rmseValBase, rmseValLR0, rmseValLR1))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"750fb391-f489-4574-9f1f-95007d6b0df2","showTitle":false,"title":""}},"source":["#### (4-IV) Grid search\n","#### We're already outperforming the baseline on the validation set by almost 2 years on average. Let's explore whether we can improve our model's performance further by conducting a grid search to determine an optimal regularization parameter. Try with `regParam` values `1e-10`, `1e-5`, and `1`."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"66356588-143e-4f64-a136-7b89e4643649","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","bestRMSE = rmseValLR1\n","bestRegParam = reg\n","bestModel = firstModel\n","\n","numIters = 500\n","alpha = 1.0\n","miniBatchFrac = 1.0\n","for reg in <FILL IN>:\n","    model = LinearRegressionWithSGD.train(parsedTrainData, numIters, alpha,\n","                                          miniBatchFrac, regParam=reg,\n","                                          regType='l2', intercept=True)\n","    labelsAndPreds = parsedValData.map(lambda lp: (lp.label, model.predict(lp.features)))\n","    rmseValGrid = calcRMSE(labelsAndPreds)\n","    print(rmseValGrid)\n","\n","    if rmseValGrid < bestRMSE:\n","        bestRMSE = rmseValGrid\n","        bestRegParam = reg\n","        bestModel = model\n","rmseValLRGrid = bestRMSE\n","\n","print(('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}\\n\\tLR1 = {2:.3f}\\n' + '\\tLRGrid = {3:.3f}').format(rmseValBase, rmseValLR0, rmseValLR1, rmseValLRGrid))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"080bf034-6782-4846-aec3-f7e3ce305087","showTitle":false,"title":""}},"source":["#### Vis. 5: Predictions of best model\n","#### We will now generate a plot similar to 'Vis. 3: Comparison of predicted vs. actual results' from Part 2. This plot will be based on the predictions made by the best model obtained from Part (4-IV) on the validation dataset. The plot will be a scatter plot that uses different colors to represent tuples comprising i) the predicted value from the best model and ii) the corresponding true label."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4e4f5666-ef96-4201-b7c0-75a47e69b672","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["predictions = np.asarray(parsedValData\n","                         .map(lambda lp: bestModel.predict(lp.features))\n","                         .collect())\n","actual = np.asarray(parsedValData\n","                    .map(lambda lp: lp.label)\n","                    .collect())\n","error = np.asarray(parsedValData\n","                   .map(lambda lp: (lp.label, bestModel.predict(lp.features)))\n","                   .map(lambda lp_tuple: squaredError(lp_tuple[0], lp_tuple[1]))\n","                   .collect())\n","\n","norm = Normalize()\n","clrs = cmap(np.asarray(norm(error)))[:,0:3]\n","\n","fig, ax = preparePlot(np.arange(0, 120, 20), np.arange(0, 120, 20))\n","ax.set_xlim(15, 82), ax.set_ylim(-5, 105)\n","plt.scatter(predictions, actual, s=14**2, c=clrs, edgecolors='#888888', alpha=0.75, linewidths=.5)\n","ax.set_xlabel('Predicted'), ax.set_ylabel(r'Actual')\n","pass"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6160ff65-a092-47b3-9fdc-bdc64d200ff4","showTitle":false,"title":""}},"source":["#### (4-V) Vary 'alpha' and number of iterations\n","#### Let's experiment with varying the value of `alpha` in the grid search. We will test two different values of `alpha`: `1e-5` and `10`. Additionally, we will train models for two different numbers of iterations: 5 and 500. After training the models, we will evaluate them on the validation set. It is important to note that if `alpha` is set too small, the gradient descent algorithm will require a large number of steps to converge to the solution, and if `alpha` is too large, it can cause numerical problems, as we will see below for `alpha = 10`."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"706cb798-e8cf-45ef-b355-cb2461ecced6","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","reg = bestRegParam\n","modelRMSEs = []\n","\n","for alpha in <FILL IN>:\n","    for numIters in <FILL IN>:\n","        model = LinearRegressionWithSGD.train(parsedTrainData, numIters, alpha,\n","                                              miniBatchFrac, regParam=reg,\n","                                              regType='l2', intercept=True)\n","        labelsAndPreds = parsedValData.map(lambda lp: (lp.label, model.predict(lp.features)))\n","        rmseVal = calcRMSE(labelsAndPreds)\n","        print('alpha = {0:.0e}, numIters = {1}, RMSE = {2:.3f}'.format(alpha, numIters, rmseVal))\n","        modelRMSEs.append(rmseVal)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5d12be8a-13f0-449d-ac8a-687c3c5e86b0","showTitle":false,"title":""}},"source":["#### Vis. 6: Heat map representation of hyperparameter\n","\n","#### Next, we perform a visualization of hyperparameter search, using a larger range of hyperparameters (with precomputed outcomes). This visualization takes the form of a heatmap, with brighter colors indicating lower RMSE values. The first plot has a large area with brighter colors, making it difficult to distinguish between the different hyperparameter values. To address this issue, we will create a second plot that focuses on the hyperparameters found within that region, allowing us to differentiate between them more easily."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b5f7ea43-55ec-4202-9044-62b60a03755a","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["from matplotlib.colors import LinearSegmentedColormap\n","\n","# Saved parameters and results, to save the time required to run 36 models\n","numItersParams = [10, 50, 100, 250, 500, 1000]\n","regParams = [1e-8, 1e-6, 1e-4, 1e-2, 1e-1, 1]\n","rmseVal = np.array([[  20.36769649,   20.36770128,   20.36818057,   20.41795354,  21.09778437,  301.54258421],\n","                    [  19.04948826,   19.0495    ,   19.05067418,   19.16517726,  19.97967727,   23.80077467],\n","                    [  18.40149024,   18.40150998,   18.40348326,   18.59457491,  19.82155716,   23.80077467],\n","                    [  17.5609346 ,   17.56096749,   17.56425511,   17.88442127,  19.71577117,   23.80077467],\n","                    [  17.0171705 ,   17.01721288,   17.02145207,   17.44510574,  19.69124734,   23.80077467],\n","                    [  16.58074813,   16.58079874,   16.58586512,   17.11466904,  19.6860931 ,   23.80077467]])\n","\n","numRows, numCols = len(numItersParams), len(regParams)\n","rmseVal = np.array(rmseVal)\n","rmseVal.shape = (numRows, numCols)\n","\n","fig, ax = preparePlot(np.arange(0, numCols, 1), np.arange(0, numRows, 1), figsize=(8, 7), hideLabels=True,\n","                      gridWidth=0.)\n","ax.set_xticklabels(regParams), ax.set_yticklabels(numItersParams)\n","ax.set_xlabel('Regularization Parameter'), ax.set_ylabel('Number of Iterations')\n","\n","colors = LinearSegmentedColormap.from_list('blue', ['#0022ff', '#000055'], gamma=.2)\n","image = plt.imshow(rmseVal,interpolation='nearest', aspect='auto',\n","                    cmap = colors)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9f3e75b3-f66c-446b-ad37-4d42e61ffccc","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# Zoom into the bottom left\n","numItersParamsZoom, regParamsZoom = numItersParams[-3:], regParams[:4]\n","rmseValZoom = rmseVal[-3:, :4]\n","\n","numRows, numCols = len(numItersParamsZoom), len(regParamsZoom)\n","\n","fig, ax = preparePlot(np.arange(0, numCols, 1), np.arange(0, numRows, 1), figsize=(8, 7), hideLabels=True,\n","                      gridWidth=0.)\n","ax.set_xticklabels(regParamsZoom), ax.set_yticklabels(numItersParamsZoom)\n","ax.set_xlabel('Regularization Parameter'), ax.set_ylabel('Number of Iterations')\n","\n","colors = LinearSegmentedColormap.from_list('blue', ['#0022ff', '#000055'], gamma=.2)\n","image = plt.imshow(rmseValZoom,interpolation='nearest', aspect='auto',\n","                    cmap = colors)\n","pass"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a3cac4ff-34fd-4edd-bdf6-cb12bf521048","showTitle":false,"title":""}},"source":["### Part 5: Adding interactions between the features (2 points)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9e147a63-8ad1-4067-b968-139b7b2ba343","showTitle":false,"title":""}},"source":["#### (5-I) 2-way interactions\n","#### Up until now, we have utilized the existing features as they were provided to us. However, we now aim to improve our model's performance by introducing additional features that capture the two-way interactions between our existing features. Write a function `twoWayInteractions` which accepts a `LabeledPoint` as input and generate a new `LabeledPoint` containing the original features as well as the two-way interactions between them. It is worth noting that if we have a dataset with three features, we would have nine possible two-way interactions ($\\scriptsize 3^2$).\n","\n","#### We can leverage the [itertools.product](https://docs.python.org/2/library/itertools.html#itertools.product) function to generate tuples for each possible two-way interaction. Furthermore, we can combine two `DenseVector` or `ndarray` objects using [np.hstack](http://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html#numpy.hstack)."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e5c3c13b-b962-4fee-b3c4-950fafe580cb","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","import itertools\n","\n","def twoWayInteractions(lp):\n","    \"\"\"Creates a new `LabeledPoint` that includes two-way interactions.\n","\n","    Note:\n","        For features [x, y] the two-way interactions would be [x^2, x*y, y*x, y^2] and these\n","        would be appended to the original [x, y] feature list.\n","\n","    Args:\n","        lp (LabeledPoint): The label and features for this observation.\n","\n","    Returns:\n","        LabeledPoint: The new `LabeledPoint` should have the same label as `lp`.  Its features\n","            should include the features from `lp` followed by the two-way interaction features.\n","    \"\"\"\n","    <FILL IN>\n","\n","print(twoWayInteractions(LabeledPoint(0.0, [2, 3])))\n","\n","# Transform the existing train, validation, and test sets to include two-way interactions.\n","trainDataInteract = <FILL IN>\n","valDataInteract = <FILL IN>\n","testDataInteract = <FILL IN>"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a17faebb-b003-4a43-a9d6-f903b3ab9f2d","showTitle":false,"title":""}},"source":["#### (5-II) Build interaction model\n","#### We will now create a model using the newly generated features. The process for creating this model is similar to what we have done before, but with some minor changes in variable names. As always, we will train the model on the training data and evaluate its performance on the validation data.\n","####  It is important to note that we need to conduct another hyperparameter search, since the best hyperparameters for our previous model may not necessarily be optimal for the new model with the added features. However, we have already set reasonable values for the hyperparameters for this exercise."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"540db108-d883-4a2b-a8b3-46796f9cdf80","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","numIters = 500\n","alpha = 1.0\n","miniBatchFrac = 1.0\n","reg = 1e-10\n","\n","modelInteract = LinearRegressionWithSGD.train(<FILL IN>, numIters, alpha,\n","                                              miniBatchFrac, regParam=reg,\n","                                              regType='l2', intercept=True)\n","labelsAndPredsInteract = <FILL IN>.map(lambda lp: (lp.label, <FILL IN>.predict(lp.features)))\n","rmseValInteract = calcRMSE(labelsAndPredsInteract)\n","\n","print(('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}\\n\\tLR1 = {2:.3f}\\n\\tLRGrid = ' + '{3:.3f}\\n\\tLRInteract = {4:.3f}').format(rmseValBase, rmseValLR0, rmseValLR1, rmseValLRGrid, rmseValInteract))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2e3a15b9-e8dd-45f5-a10f-f7ef318ae9d2","showTitle":false,"title":""}},"source":["#### (5-III) Evaluate interaction model on test set\n","#### To assess the performance of our model on new data, we will evaluate it on the test set. It is important to note that we did not use the test set to evaluate any of our models during the development phase. Therefore, the test set provides an unbiased estimate of the performance of our model on new data. If we had changed our model based on viewing its performance on the test set, our estimate of RMSE would likely be overly optimistic.\n","#### We will also calculate the RMSE for both the baseline model and our new model. This will allow us to quantify the improvement in performance that our model provides over the baseline model."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4f6eb9a0-fe4c-47d8-9acb-44791f87d992","showTitle":false,"title":""},"collapsed":false},"outputs":[],"source":["# TODO: Replace <FILL IN> with appropriate code\n","labelsAndPredsTest = <FILL IN>\n","rmseTestInteract = <FILL IN>\n","\n","print(('Test RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLRInteract = {1:.3f}'.format(rmseTestBase, rmseTestInteract)))"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Assignment_3_Linear_Regression_Spark","notebookOrigID":4166122097223279,"widgets":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10"},"vscode":{"interpreter":{"hash":"bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"}}},"nbformat":4,"nbformat_minor":0}
